{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "litellm._turn_on_debug()\n",
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, \n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', \n",
    "                    handlers=[logging.FileHandler('edgar.log', 'w', 'utf-8')])\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def query_litellm(prompt: str, model_name: str='ollama/hf.co/ernanhughes/Fin-R1-Q8_0-GGUF') -> str:\n",
    "    \"\"\"\n",
    "    Query a LiteLLM-compatible model with a given prompt.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The user's input question.\n",
    "        model_name (str): The model to use (e.g. 'ollama/qwen2.5' or 'huggingface/your-model').\n",
    "\n",
    "    Returns:\n",
    "        str: The model's response text.\n",
    "    \"\"\"\n",
    "    response = litellm.completion(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    return response['choices'][0]['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_context_limit(token_multiplier=50):\n",
    "    base = \"The quick brown fox jumps over the lazy dog. \"\n",
    "    for i in range(1, 1000):\n",
    "        try:\n",
    "            prompt = base * (i * token_multiplier)\n",
    "            print(f\"Trying length {len(prompt)}...\")\n",
    "            query_litellm(prompt)  # or however you call your local model\n",
    "        except Exception as e:\n",
    "            print(f\"Failed at length {len(prompt)}: {e}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from huggingface_hub import HfApi\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def config_to_markdown(config_dict):\n",
    "    # Priority ordered keys\n",
    "    priority_keys = [\n",
    "        \"model_type\", \"architectures\", \"hidden_size\", \"num_attention_heads\",\n",
    "        \"num_hidden_layers\", \"intermediate_size\", \"vocab_size\", \"type_vocab_size\",\n",
    "        \"max_position_embeddings\", \"hidden_act\", \"initializer_range\",\n",
    "        \"layer_norm_eps\", \"attention_probs_dropout_prob\", \"hidden_dropout_prob\",\n",
    "        \"pad_token_id\", \"bos_token_id\", \"eos_token_id\"\n",
    "    ]\n",
    "    all_keys = list(config_dict.keys())\n",
    "    \n",
    "    # Sort by priority first, then append any other keys\n",
    "    ordered_keys = [k for k in priority_keys if k in config_dict] + \\\n",
    "                   [k for k in all_keys if k not in priority_keys]\n",
    "\n",
    "    # Create markdown table\n",
    "    markdown = \"| Key | Value |\\n|-----|-------|\\n\"\n",
    "    for k in ordered_keys:\n",
    "        v = config_dict[k]\n",
    "        if isinstance(v, list):\n",
    "            v = \", \".join(map(str, v))\n",
    "        elif isinstance(v, dict):\n",
    "            v = json.dumps(v)\n",
    "        markdown += f\"| `{k}` | `{v}` |\\n\"\n",
    "    return markdown\n",
    "\n",
    "def inspect_model_config(model_name_or_path, source=\"autoconfig\"):\n",
    "    \"\"\"\n",
    "    Inspect a Hugging Face model's configuration and output as markdown.\n",
    "\n",
    "    Parameters:\n",
    "        model_name_or_path (str): model name from Hugging Face hub or local path.\n",
    "        source (str): One of [\"autoconfig\", \"api\", \"rest\", \"local_json\"]\n",
    "    \"\"\"\n",
    "    config_dict = None\n",
    "\n",
    "    if source == \"autoconfig\":\n",
    "        print(\"ðŸ” Using AutoConfig (local or remote)...\")\n",
    "        config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "        config_dict = config.to_dict()\n",
    "\n",
    "    elif source == \"api\":\n",
    "        print(\"ðŸŒ Using huggingface_hub HfApi...\")\n",
    "        api = HfApi()\n",
    "        model_info = api.model_info(model_name_or_path)\n",
    "        config_dict = model_info.config if hasattr(model_info, 'config') else {}\n",
    "\n",
    "    elif source == \"rest\":\n",
    "        print(\"ðŸŒ Using Hugging Face REST API...\")\n",
    "        url = f\"https://huggingface.co/api/models/{model_name_or_path}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            config_dict = data.get(\"config\", {})\n",
    "        else:\n",
    "            print(\"âŒ Failed to retrieve model metadata from REST API.\")\n",
    "\n",
    "    elif source == \"local_json\":\n",
    "        print(\"ðŸ“‚ Loading config.json from local directory...\")\n",
    "        path = os.path.join(model_name_or_path, \"config.json\")\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\") as f:\n",
    "                config_dict = json.load(f)\n",
    "        else:\n",
    "            print(f\"âŒ config.json not found at {path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"âš ï¸ Invalid source. Choose from: autoconfig, api, rest, local_json.\")\n",
    "\n",
    "    if config_dict:\n",
    "        print(\"\\n### ðŸ“„ Model Configuration (Markdown Table)\\n\")\n",
    "        print(config_to_markdown(config_dict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Using AutoConfig (local or remote)...\n",
      "\n",
      "### ðŸ“„ Model Configuration (Markdown Table)\n",
      "\n",
      "| Key | Value |\n",
      "|-----|-------|\n",
      "| `model_type` | `qwen2` |\n",
      "| `architectures` | `Qwen2ForCausalLM` |\n",
      "| `hidden_size` | `3584` |\n",
      "| `num_attention_heads` | `28` |\n",
      "| `num_hidden_layers` | `28` |\n",
      "| `intermediate_size` | `18944` |\n",
      "| `vocab_size` | `152064` |\n",
      "| `max_position_embeddings` | `32768` |\n",
      "| `hidden_act` | `silu` |\n",
      "| `initializer_range` | `0.02` |\n",
      "| `pad_token_id` | `None` |\n",
      "| `bos_token_id` | `151643` |\n",
      "| `eos_token_id` | `151645` |\n",
      "| `use_sliding_window` | `False` |\n",
      "| `sliding_window` | `None` |\n",
      "| `max_window_layers` | `28` |\n",
      "| `num_key_value_heads` | `4` |\n",
      "| `rms_norm_eps` | `1e-06` |\n",
      "| `use_cache` | `True` |\n",
      "| `rope_theta` | `1000000.0` |\n",
      "| `rope_scaling` | `None` |\n",
      "| `attention_dropout` | `0.0` |\n",
      "| `return_dict` | `True` |\n",
      "| `output_hidden_states` | `False` |\n",
      "| `output_attentions` | `False` |\n",
      "| `torchscript` | `False` |\n",
      "| `torch_dtype` | `bfloat16` |\n",
      "| `use_bfloat16` | `False` |\n",
      "| `tf_legacy_loss` | `False` |\n",
      "| `pruned_heads` | `{}` |\n",
      "| `tie_word_embeddings` | `False` |\n",
      "| `chunk_size_feed_forward` | `0` |\n",
      "| `is_encoder_decoder` | `False` |\n",
      "| `is_decoder` | `False` |\n",
      "| `cross_attention_hidden_size` | `None` |\n",
      "| `add_cross_attention` | `False` |\n",
      "| `tie_encoder_decoder` | `False` |\n",
      "| `max_length` | `20` |\n",
      "| `min_length` | `0` |\n",
      "| `do_sample` | `False` |\n",
      "| `early_stopping` | `False` |\n",
      "| `num_beams` | `1` |\n",
      "| `num_beam_groups` | `1` |\n",
      "| `diversity_penalty` | `0.0` |\n",
      "| `temperature` | `1.0` |\n",
      "| `top_k` | `50` |\n",
      "| `top_p` | `1.0` |\n",
      "| `typical_p` | `1.0` |\n",
      "| `repetition_penalty` | `1.0` |\n",
      "| `length_penalty` | `1.0` |\n",
      "| `no_repeat_ngram_size` | `0` |\n",
      "| `encoder_no_repeat_ngram_size` | `0` |\n",
      "| `bad_words_ids` | `None` |\n",
      "| `num_return_sequences` | `1` |\n",
      "| `output_scores` | `False` |\n",
      "| `return_dict_in_generate` | `False` |\n",
      "| `forced_bos_token_id` | `None` |\n",
      "| `forced_eos_token_id` | `None` |\n",
      "| `remove_invalid_values` | `False` |\n",
      "| `exponential_decay_length_penalty` | `None` |\n",
      "| `suppress_tokens` | `None` |\n",
      "| `begin_suppress_tokens` | `None` |\n",
      "| `finetuning_task` | `None` |\n",
      "| `id2label` | `{\"0\": \"LABEL_0\", \"1\": \"LABEL_1\"}` |\n",
      "| `label2id` | `{\"LABEL_0\": 0, \"LABEL_1\": 1}` |\n",
      "| `tokenizer_class` | `None` |\n",
      "| `prefix` | `None` |\n",
      "| `sep_token_id` | `None` |\n",
      "| `decoder_start_token_id` | `None` |\n",
      "| `task_specific_params` | `None` |\n",
      "| `problem_type` | `None` |\n",
      "| `_name_or_path` | `SUFE-AIFLM-Lab/Fin-R1` |\n",
      "| `_attn_implementation_autoset` | `False` |\n",
      "| `transformers_version` | `4.49.0` |\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m model_name = \u001b[33m\"\u001b[39m\u001b[33mSUFE-AIFLM-Lab/Fin-R1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      2\u001b[39m model_config = inspect_model_config(model_name, source=\u001b[33m\"\u001b[39m\u001b[33mautoconfig\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mconfig_to_markdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mconfig_to_markdown\u001b[39m\u001b[34m(config_dict)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconfig_to_markdown\u001b[39m(config_dict):\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# Priority ordered keys\u001b[39;00m\n\u001b[32m      9\u001b[39m     priority_keys = [\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel_type\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marchitectures\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhidden_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mnum_attention_heads\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnum_hidden_layers\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mintermediate_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mvocab_size\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype_vocab_size\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpad_token_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mbos_token_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33meos_token_id\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     all_keys = \u001b[38;5;28mlist\u001b[39m(\u001b[43mconfig_dict\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m())\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Sort by priority first, then append any other keys\u001b[39;00m\n\u001b[32m     19\u001b[39m     ordered_keys = [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m priority_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m config_dict] + \\\n\u001b[32m     20\u001b[39m                    [k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m all_keys \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m priority_keys]\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "model_name = \"SUFE-AIFLM-Lab/Fin-R1\"\n",
    "model_config = inspect_model_config(model_name, source=\"autoconfig\")\n",
    "config_to_markdown(model_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
